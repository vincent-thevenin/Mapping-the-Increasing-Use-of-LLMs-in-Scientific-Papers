{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run simple inference on a single PDF file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"CS\"  # distribution against which to compare\n",
    "pdf_path = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PDF data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install spacy's large english model\n",
    "! python -m spacy download en_core_web_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import re\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "def tokenize(text):\n",
    "    \"\"\"\n",
    "    Processes the input text, splits it into sentences, and further processes each sentence\n",
    "    to extract non-numeric words. It constructs a list of these words for each sentence.\n",
    "\n",
    "    Parameters:\n",
    "    text (str): A string containing multiple sentences.\n",
    "\n",
    "    Returns:\n",
    "    list: A list of lists, where each inner list contains the words from one sentence,\n",
    "          excluding any numeric strings.\n",
    "    \"\"\"\n",
    "    # remove newline characters, this line is not necessary for all cases\n",
    "    # the reason it is included here is because the abstracts in the dataset contain abnormal newline characters\n",
    "    # e.g. Recent works on diffusion models have demonstrated a strong capability for\\nconditioning image generation,\n",
    "    text=text.replace('\\n',' ')\n",
    "    # Initialize an empty list to store the list of words for each sentence\n",
    "    sentence_list=[]\n",
    "    # Process the sentence using the spacy model to extract linguistic features and split into components\n",
    "    doc=nlp(text)\n",
    "    # Iterate over each sentence in the processed text\n",
    "    for sent in doc.sents:\n",
    "        # Extract the words from the sentence\n",
    "        words = re.findall(r'\\b\\w+\\b', sent.text.lower())\n",
    "        # Remove any words that are numeric\n",
    "        words_without_digits=[word for word in words if not word.isdigit()]\n",
    "        # If the list is not empty, append the list of words to the sentence_list\n",
    "        if len(words_without_digits)!=0:\n",
    "            sentence_list.append(words_without_digits)\n",
    "    return sentence_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pypdf import PdfReader\n",
    "\n",
    "reader = PdfReader(pdf_path)\n",
    "number_of_pages = len(reader.pages)\n",
    "text = ''\n",
    "for i in range(number_of_pages):\n",
    "    page = reader.pages[i]\n",
    "    text += page.extract_text()\n",
    "    text += '\\n'\n",
    "\n",
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_sentences = tokenize(text)\n",
    "print(tokenized_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save text to parquet with column: 'inference_sentence'\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({'inference_sentence': tokenized_sentences})\n",
    "df.to_parquet('tokenized_sentences.parquet', index=False)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.estimation import estimate_text_distribution\n",
    "from src.MLE import MLE\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subjects = [\"CS\",\"EESS\",\"Math\",\"Phys\",\"Stat\"]\n",
    "assert name in subjects, \"Data not available for source subject\"\n",
    "\n",
    "if not os.path.exists(f\"distribution/{name}.parquet\"):\n",
    "    estimate_text_distribution(f\"data/training_data/{name}/human_data.parquet\",f\"data/training_data/{name}/ai_data.parquet\",f\"distribution/{name}.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLE(f\"distribution/{name}.parquet\")\n",
    "model.inference(\"tokenized_sentences.parquet\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
